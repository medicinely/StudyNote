---
title: 机器学习算法
date: 2020-10-12 10:14:38
tags:

---


> 

# 第一章 数据集

## 1.数据类型

- 离散型数据（分类）
  - 由记录不同类别个体的数目所得到的数据，`称为计数数据`，所有这些数据全部都是整数，而且不能再细分，也不能进一步提高他们的精确度
- 连续型数据（回归）
  - 变量可以在`某个范围内取任一数`，即变量的取值可以是连续的，如：`长度、时间、质量值等`，这类整数通常是非整数，含小数部分
- 注：**离散型是区间内不可分，连续型是区间内可分**

## 2.机器学习算法分类

- 监督学习（预测）

  > 输入数据有特征标签，即有标准答案

  - ​	分类（目标值离散型）
    - k-近邻算法
    - 贝叶斯分类
    - 决策树与随机森林
    - 逻辑回归
    - 神经网络
  - 回归（目标值连续型）
    - 线性回归
    - 岭回归
  - 标注
    - 隐马尔可夫模型

- 无监督学习

  > 输入数据有特征无标签，即无标准答案

  - 聚类
    - k-means

# 第二章 sklearn介绍

## 1.数据集划分

- ```markdown
  * from sklearn.model_selection import train_test_split(*arrays,**options)
  	
  	* x 数据集的特征值
  	* y 数据集的标签值(目标集)
  	* test_size 测试集的大小，一般为float(比例)
  	* random_state 随机数种子，不同的种子会造成不同的随机采样结果，相同的种子采样结果相同
  	* return	
  		* 训练集特征值，测试集特征集，训练标签(目标集)，测试标签(目标集)（全部默认随机取）
  ```

## 2.数据集获取

- **获取数据api**

  - ```markdown
    sklearn.datasets
    
    - 加载获取流行数据集
      - datasets.load_*()
        - 获取小规模数据集，数据包含在datasets里
      - datasets.fetch_*(data_home=None)
      - 获取大规模数据集，需要从网络上下载，函数的第一个参数是data_home,表示数据集下载的目录，默认是~/scikit_learn_data/
    ```

- **load* 和 fetch* 返回的数据类型（字典格式）**

  - ```markdown
    * datasets.base.Bunch(字典格式)
    	* data:特征数据数组（特征集）
    		* 是[n_samples,n_features]的二位numpy.ndarray数组
    	* target：标签数组（目标集）
    		* 是n_samples的一维numpy.ndarray数组
    	* DESCR：数据描述
    	* feature_names：特征名
    		* 新闻数据，手写数字（不包含回归数据集）
    	* target_names：标签名
    ```

- **sklearn分类数据集**

  - sklearn.datasets.load_iris()

    - 加载并返回鸢尾花数据集

      - | 名称         | 数量 |
        | ------------ | ---- |
        | 类别         | 3    |
        | 特征         | 4    |
        | 样本数量     | 150  |
        | 每个类别数量 | 50   |

        

  - sklearn.datasets.load_digits()

    - 加载并返回数字数据集

      - | 名称     | 数量 |
        | -------- | ---- |
        | 类别     | 10   |
        | 特征     | 64   |
        | 样本数量 | 1797 |

  - sklearn.datasets.fetch_20newsgroups()

    - ```markdown
      * subset： 选择要加载的数据集
      	* 'train'或者'test','all'，可选
      ```

  

- **sklearn回归数据集**

  - sklearn.datasets.load_boston()

    - 加载并返回波士顿房价数据集

    - | 名称     | 数量 |
      | -------- | ---- |
      | 目标类型 | 5-50 |
      | 特征     | 13   |
      | 样本数量 | 506  |

  - sklearn.datasets.load_diabetes()

    - 加载和返回糖尿病数据集

      - | 名称     | 数量   |
        | -------- | ------ |
        | 目标范围 | 25-348 |
        | 特征     | 10     |
        | 样本数量 | 442    |

      

- **sklearn清除目录下的数据**

  - datasets.clear_data_home(data_home=None)

    

## 3.转换器和估计器

### 3.1 转换器

> **实现了特征工程的api**

- 特征工程步骤
  - 1.实例化（实例化就是一个`转换器（Transformer）`）
  - 2.调用fit_transform(`对于文档建立分类词频矩阵，不能同时调用`) ==》**执行平均值和标准差**
  - 【数据集】 ====》【转换后的数据集】( fit_transform(x) )
- fit_transtorm()：输入数据直接转换
  - fit(x,y)：输入数据，但不转换  ===》计算平均值，方差等（建立标准）
  - transform(x,y)：进行数据的转换

### 3.2 估计器

> 在sklearn中，估计器（estimator）是一个重要的角色，**是一类实现算法的API**

- 1.用于分类的估计器
  - sklearn.neighbors	k-近邻算法
  - sklearn.naiva_bayes     贝叶斯算法
  - sklearn.linear_model.LogisticRegression    逻辑回归
  - sklearn.tree     决策树和随机森林
- 2.用于回归的估计器
  - sklearn.linear_model.LinearRegression     线性回归
  - sklearn.linear_model.Ridge    岭回归

 **流程**

- 1.数据集{训练集(x_train,y_taian)，测试集(x_test,y_test)}
- 2.特征工程
  - 调用fit(x_train,y_train)
- 3.估计结果
  - y_predict = predict(x_test)  预测结果
  - 预测的准确率：score(x_test,y_test)



# 第三章 算法(实例基于sklearn)

> cikit-learn优点：封装好，建立模型简单，预测简单
>
> ​				 缺点：算法的过程中有些参数都在算法API内部优化
>
> tensorflow：封装高低，可以自己实现线性回归，学习率等等

## 1.1 K-近邻算法

> 如果一个样本在特征空间中的`k个最相似（即特征空间中最邻近的样本中的大多数属于某个类别）`则该样本也属于这个类别

**欧式距离：**相似的样本，特征之间的值应该很接近

> 样本a：a(a1,a2,a3)
>
> 样本b：b(b1,b2,b3)

$$
\sqrt{(a1-b1)^2 + (a2-b2)^2 + (a3-b3)^2}
$$

### 1.2 实例一

**[预测入住位置](https://www.kaggle.com/c/facebook-v-predicting-check-ins/data)**

```vim
特征值：x,y坐标
		定位准确性
		时间
目标值：入住的位置ID
```

```markdown
* k值取多大？有什么影响
	* k值取很小：容易受异常点影响
	* k值取很大：容易受k值数量（类别）波动
	
* 性能问题？
	* 性能较低
	
* k-近邻算法优缺点
	* 优点
		*简单，易于理解，易于实现，无需估计参数，无需训练
	* 缺点
		* 懒惰算法，对测试样本分类时的计算量大，内存开销大
		* 必须指定k值，k值选择不当则分类精度不能保证

* 使用场景：小数据场景，几千～几万样本，具体场景具体业务去测试
```



##　1.2 朴素贝叶斯算法

> 使用前提：特征独立

### 1.2.1 统计学

```vim
条件：所有特征之间是条件独立

联合概率：包含多个条件，且所有条件同时成立的概率
	记作：Ｐ(A,B)
		  Ｐ(A,B) = Ｐ(A)Ｐ(B)
条件概率：就是事件Ａ在另外一个事件Ｂ已经发生条件下的发生概率
	记作：Ｐ(A|B)
		  Ｐ(A1,A2|B) = Ｐ(A1|B)Ｐ(A2|B)
	注意：此条件概率的成立，是由于Ａ1,A2相互独立的结果
```

### 1.2.2 算法例子

```vim
* 前提 特征独立
	* P(科技｜词１,词２,词３...)	文档１：词１，词２，词３,...
	* P(娱乐|词a,词ｂ...)	文档２：词a，词b，词c,...
```

### 1.2.3 贝叶斯原理

```vim
- C 为类别集合，其中每一个元素是一个类别
- W 为特征值
```


$$
Ｐ(C|W) = \frac{P(W|C)P(C)}{P(W)}
$$

**形象表示：**
$$
Ｐ(类别|特征) = \frac{P(特征|类别)P(类别)}{P(特征)}
$$


* 注：ｗ为给定文档的特征值（频数统计，预测文档提供），ｃ为文档类别

  

* **形象表示：**假设在某文档中，存在多个分类（科技，农业，天文）,每个分类存在多个特征值（词1，词2...）
  $$
  P(科技｜词1,词2,词3) = \frac{P(词1,词2,词3|科技)P(科技)}{P(词1,词2,词3)}=\frac{P(词1|科技)P(词2|科技)P(词3|科技)P(科技)}{P(词1|科技)P(词2|科技)P(词3|科技)}
  $$
  

**例子代入**

```vim
* P(科技｜词１,词２,词３...) = P(F1,F2,F3|科技)P(科技)/P(W)
* P(娱乐｜词a,词b,...) = P(F1,F2,F3|娱乐)P(娱乐)/P(W)
```

```
* 公式分为三部分
	* P(C):每个类别的概率（某类别个数/总类别数量）
	* P(W|C)：给定类别下特征（被预测文档中出现的词）的概率
		* 计算方法：　Ｐ(F1|C) = Ni/N	(训练文档中去计算)
			Ｎｉ为Ｆ１词在Ｃ类别所有文档中出现的次数
			Ｎ为所有类别Ｃ下的文档所有词出现的次数和
	* Ｐ(F1,F2,...)：预测文档中每个词的概率
	
* 拉普拉斯平滑
	* 原因：有可能某个特征对应的类别为０，会出现不合理的概率
	* 解决：拉普拉斯平滑系数
		* a 为指定的系数一般为１，ｍ为训练文档中统计出的特征词个数
```

$$
拉普拉斯平滑:P(F1|C) = \frac{Ni+a}{N+am}
$$

**实例讲解：**

```markdown
* 假设在文本分类中，有3个类，C1、C2、C3，在指定的训练样本中
* 在某个词语K1，在各个类中观测计数分别为0,99,10
* K1的概率为 0、0.99、0.01，对于这三个量使用拉普拉斯平滑的计算方法如下：
```

$$
\frac{0+1}{1000+3}=0.001 ，  \frac{990+1}{1000+3}=0.988，\frac{10+1}{1000+3}=0.011
$$



### 1.2.4　sklearn中API实现

> sklearn.naive_bayes.MultinomialNB

```python 
sklearn.naive_bayes.MultinomialNB(alpha=1.0)
	#朴素贝叶斯分类
    #alpha：拉普拉斯平滑系数
```

### 1.2.5 实例一

```vim
from sklearn.datasets import load_iris,fetch_20newsgroups
sklearn２０类新闻分类
２０个新闻组数据集包含２０个主题的18000个新闻组帖子
```

```vim
# 朴素贝叶斯算法特点

    * 优点
        * 朴素贝斯模型发源于古典数学理论，有稳定的分类效率
        * 对缺失数据不太敏感，算法也比较简单，常用于文本分类
        * 分类准确度高，速度快
    * 缺点
        * 由于使用样本属性独立性的假设，所以如果样本属性有关联时，其效果不好
            ×　假设了文章当中一些词语另外一些是独立没关系的　　不太靠谱
            ×　训练集当中去进行统计词这些工作　　　　　　会对结果造成干扰
```

## 1.3 决策树

> 信息和消除不确定性是相联系的

### 1.3.1 实例一

**以球队作为讲解**

```markdown
* 现有３２支球队，猜测某支球队获胜的机会
---

* ３２支球队递归均分　 -1~16- | -17~32- 	
	－－　不知道任何一个球队的信息（猜对的可能需要次数）：log32=5比特(代价:`信息熵`)
		*　每支队伍的获胜的概率：1/32
		*  5 = -(1/32log1/32 + 1/32log1/32 ...)  ==>(加３２次)
	－－　开放一些数据信息（猜对的可能需要次数）：
		*  德国（１/6）、巴西（１/6）、中国(１/10) 
		*  5 > -(1/6log1/6 + 1/6log1/6 ...)		信息熵下降
		
**信息熵：**	

* 谁是世界杯冠军的信息量应该比５比特小，香农（科学家）指出，它的准确信息量应该是：
    * H = -(Ｐ１logＰ１＋Ｐ２logＰ２＋...＋Ｐ３２logＰ３２)
    * H 的专业术语称之为`信息熵`，单位为比特
    * 当这３２支队伍夺冠的机率相同时，对应的信息熵等于５比特
```

**信息熵公式：**
$$
Ｈ(x) = \sum_{}x\epsilon{X^{P(x)logP(x)}}
$$

### 1.3.2 信息增益

> 决策数的划分依据之一

```markdown
**信息增益，即公式为：

* ｇ(D,A) = H(D) - H(D|A)
	* ｇ(D,A)：`特征Ａ对训练数据集Ｄ的信息增益`
	* H(D)：`初始信息熵大小`
	* Ｈ(D|A)：`特征Ａ给定条件下Ｄ的信息条件熵`
*注：信息增益表示得知特征Ｘ的信息而使得类Ｙ的信息的不确定性减少的程度（减少信息熵的大小）
```

信息增量：当得知一个特征条件之后，减少的信息熵的大小

**实例：**

​	**银行贷款数据**

| ＩＤ | 年龄            | 有工作   | 有房子   | 信贷情况 | 类别       |
| ---- | --------------- | -------- | -------- | -------- | ---------- |
|      | 青(5)中(5)老(5) | 是()否() | 是()否() |          | 是(9)否(6) |
| １   | 青年            | 否       | 否       | 一般     | 否         |
| ２   | 青年            | 是       | 是       | 良好     | 是         |
| ３   | 中年            | 是       | 是       | 良好     | 是         |
| ４   | 老年            | 否       | 是       | 差       | 否         |
| ...  |                 |          |          |          |            |



**当前类别的信息熵大小：**
$$
H(D) = -(9/15log9/15 + 6/15log6/15)　＝　0.971
$$
​	**年龄：**
$$
g(D,年龄)　＝　Ｈ(D) - H(D’|年龄)　＝ 0.971 -[\frac{1}{3}H(青年)＋\frac{1}{3}H(中年)＋\frac{1}{3}H(老年)]
$$
​	青年对应的类别（五个青年，类别`是`有２个，`否`有３个）：
$$
Ｈ(青年)　＝　-(\frac{2}{5}log\frac{2}{5}+\frac{2}{5}log\frac{2}{5})
$$
​	中年对应的类别（五个中年，类别`是`有２个，`否`有３个）：
$$
Ｈ(中年)　＝　-(\frac{2}{5}log\frac{2}{5}+\frac{2}{5}log\frac{2}{5})
$$
​	老年对应的类别（五个中年，类别`是`有４个，`否`有１个）：
$$
Ｈ(老年)　＝　-(\frac{４}{5}log\frac{４}{5}+\frac{１}{5}log\frac{１}{5})
$$
**最终的计算结果：**

```markdown
g(D,年龄)　＝　0.311(假的)
g(D,工作)　＝　0.324
g(D,房子)　＝　0.420
g(D,信贷)　＝　0.363
`最终结论`：**相比较特征：有自己房子的信息增益量大,所以选择特征房子作为最有特征(确定决策树的特征排序顺序)　
```

### 1.3.3 常见决策树使用的算法

- ID3：信息增量益最大的准则
- C4.5：信息增益比，最大的准则
- CART
  - 同归树：平方误差　最小
  - 分类树：基尼系数，最小的准则　在sklearn中可以选择划分的默认原则

### 1.3.4 sklearn决策树API

> [泰坦尼克号数据](http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt)

```markdown
sklearn.tree.DecisionTreeClassifier(
	criterion='gini',
	max_depth=None,
	random_state=None
	)
	
* 决策树分类器
    * criterion：默认是'gini'系数，也可以选择信息增益的熵'entropy'
    * max_depth：树的深度大小
    * random_state: 随机数种子

* 方法
	* decision_path：返回决策树的路径
```

### 1.3.5 决策树的结构、本地保存

```markdown
* 1.sklearn.tree.export_graphviz()该函数能导出ＤＯＴ格式
	tree.export_graphviz(estimator,out_file='tree.dot',feature_names=[','])
* 2.工具（能够将ｄｏｔ文件转换成ｐｄｆ,ｐｎｇ格式）
	×　apt安装　graphviz
* 3.运行命令
	×　dot -Tpng tree.dot -o tree.png
```

### 1.3.6 决策树的优缺点

* 优点
  * 简单的理解和解析，树木可视化
  * 需要很少的数据准备，其他技术通常需要数据归一化
* 缺点
  * 决策树学习者创建过于复杂的树，不能很好推广（过拟合）
* 改进
  * 减枝cart算法（决策树API当中已经实现）
    * 参数1：min_samples_split=2
    * 参数2：min_samples_leaf=1
  * 随机森林
* 注：企业重要决策，由于决策树有很好的分析能力，在决策过程应用中较多



## 1.4 随机森林

> 集成学习中的一种
>
> 在机器学习中，随机森林是一个包含多个决策树的分类器，并且其输出的类别是有个别树输出的类别的众数而定（人话：多颗决策树中投票决定类别结果）

**集成学习方法：**

```markdown
集成学习通过建立几个模型组合的来解决单一预测问题。
它的工作原理是生成多个`分类器/模型`，各自独立的学习和做出预测。
这些预测最后结合成单预测，因此由于任何一个单分类的做出预测
```

### 1.4.1 随机森林形成过程

> 特点：随机有返回的抽样（bootstrap抽样）
>
> 假设：N个样本，M个特征

* 单个树建立过程
  * 1. `随机`在N个样本当中选择一个样本，重复N次（样本可能重复）
  * 2. `随机`在M个特征当中选出m个特征（M>m）

* 建立多个决策树，样本，特征大多不一样

### 1.4.2 解惑

* 为什么要随机抽样训练集

  * ```vim
    如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样
    ```

* 为什么要有放回地抽样

  * ```markdown
      如果不是有放回抽样，那么每棵树的训练样本都是不同的，都是没有交集，
    这样每棵树都是`有偏差的`，都是可能`片面的`，也就是说每棵树训练出来都是有很大的差异的，
    而随机森林最后分类取决于多棵树（`弱分类器`）的投票表决。
    ```

### 1.4.3随机森林API

```markdown
sklearn.ensemble.RandomForestClassifier(
	n_estimators=10,
	criterion='gini',
	max_depth=None,
	bootstrap=True,
	random_state=None,
	)
* 随机深林分类器（超参数）
	* n_estimators:森林里树木的数量，可选(默认=10)
		* 建议值：120,200,300,500,800,1200  
	* criteria：分割特征的测量方法，可选（默认='gini'）	
	* max_depth：树的最大深度，可选（默认=None）
		* 建议值：5,8,15,25,30
	* max_features：每个决策树的最大特征数量（默认='auto'）
		* auto：sqrt(总特征数)
		* log2: log2(总特征数)
		* None: 总特征数
    * bootstrap：是否构建树时使用放回抽样（默认=True）
```

### 1.4.4 优势

* 在当前所有算法中，具有极好的准确率
* 能够有效地运行在大数据集上
* 能够处理具有高维特征的输入样本，而且不需要降维
* 能够评估各个特征在分类问题上的重要性

## 1.5 线性回归

> 定义：线性回归通过一个或多`自变量`与`因变量`之间进行建模的线性回归分析
>
> ​			其中可以为`一个或多个自变量之间的线性组合（线性回归的一种）`

> 回归：目标值连续
>
> 分类：离散型

```markdown
线性回归：寻找一种能预测的趋势
---

* 线性关系定义：
	y = kx + b
* 线性关系模型
	× 一个通过`属性的线性组合`来进行预测的函数：
	* w为权重，b称为偏置项
```

$$ {线性回归}
f(x) = w_1x_1 + w_2x_2 + ... + w_n+x_n + b = w^Tx
$$

$$
其中w，x为矩阵：
w = \begin{pmatrix} w_0\\w_1\\w_2 \end{pmatrix} ，x = \begin{pmatrix} x_0\\x_1\\x_2 \end{pmatrix}
$$

* 在应用上中使用为矩阵（必须二维）
  * 为了满足了特定运算需求
    * 矩阵乘法（m行，I列）* (I行，n列) = （m行，n列） 
      * **注：当矩阵A的列数（column）等于矩阵B的行数（row）时，A与B可以相乘。**
      * 结论：特征值*权重=目标值

### 1.5.1 损失函数（误差）

* y_i 为第i个训练样本的真实值

* h_w *x_i为第i个训练样本特征值组合预测函数

* 总损失定义（**最小二乘法**）：

	* $$
  f(\theta) = （h_wx_1-y_1）^2 + (h_wx_2)^2 + ...+(h_wx_m-y_m)^2= \sum_{i=1}^m (h_wx_i-y_i)^2
  $$

  

### 1.5.2 最佳模型（优化）

> 在线性回归中：
> $$
> f(x) = w_1x_1 + w_2x_2 + ... + w_n+x_n + b寻找模型中的w，使得损失最小
> $$



**寻找模型中的w，使得损失最小: **



* **1.最小二乘法之正规方程**

  * $$
    w= (x^Tx)^{-1}x^Ty
    $$

  * x为特征值矩阵，y为目标值矩阵，T转置（行变列，列变行）， -1矩阵的逆
    
    * **缺点：当特征过于复杂，求解速度太慢**
    
    

* **2.最小二乘法之梯度下降法(重点)**

  * 以单变量中的w0,w1为例子：

    * $$
      w_1 := -w_1 - \alpha \frac{\alpha  cost(w_0+w_1x_1)}{\alpha w_1}
      $$

      $$
      *w_0 := -w_0 - \alpha \frac{\alpha cost(w_0+w_1x_1)}{\alpha w_1}
      $$

    * a 为学习速率（`手动指定`），代表方向：
      $$
      \frac{\alpha cost(w_0+w_1x_1)}{\alpha w_1}
      $$

      * 理解：沿着这个函数下降的方向找，最后就能找到山谷的最低点，然后`更新w值`

    * 使用场景：面对训练数据规模十分庞大的任务

### 1.5.3 线性回归正规方程、梯度下降API

* 正规方程：sklearn.linear_model.LinearRegression
  *  普通最小二乘法线回归
  * coef_：回归系数
* 梯度下降：sklearn.linear_model.SGDRegressor
  * 通过使用SGD最小化线性模型
  * coef_：回归系数

| 梯度下降                     | 正规方程                                                     |
| ---------------------------- | ------------------------------------------------------------ |
| 需要选择学习率ａ             | 不需要                                                       |
| 需要多次迭代                 | 一次运算得到                                                 |
| 当特征数量ｎ大时也能较好适用 | 需要计算（ｘ^T * X）^-1,<br />如果特征数量ｎ较大则运算代价大，<br />因为矩阵逆的计算时间复杂度为Ｏ(n2),<br />通常来说当ｎ小于10000时还是可以接受 |
| 适用于各种类型的模型         | 只适用于线性模型，不适用逻辑回归模型等其他模型               |



### 1.5.4 回归性能评估

* 均方误差（Mean Squared Error）MSE评价机制：

  * $$
    MSE = \frac{1}{m}\sum_{i=1}^m(y^i-\breve{y})^2
    $$

    $$
    注：ｙ^i为预测值，\breve{y}为真实值
    $$

* sklearn回归评估ＡＰＩ

  * sklearn.metrics.mean_squared_error(y_true,y_pred)
    * 均分误差回归损失
      * y_true ：真实值
      * y_pred：预测值
      * return：浮点数结果
    * 注：真实值，预测值为标准化之前的值

## 1.6 岭回归

> Ridge:岭回归，带有正则化的线性回归，解决过拟合

* L2正则化：针对的是线性回归
  * 作用：可以使得w(权重)的每个元素都很小，都接近于0
  * 优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象

### 1.61 带正则化的线性回归：Ridge

- sklearn.linear_model.Ridge

  - ```markdown
    * sklearn.linear_model.Ridge(alpha=1.0)
    	* 具有L2正则化的线性最小二乘法
    		* alpha：正则化力度 λ
    		* coef_：回归系数
    ```



### 1.6.2 线性回归LinearRegression和Ridge对比

* 岭回归
  * 回归得到的`回归系数更符合实际，更可靠`。另外，能让估计参数的波动范围变小，变的更稳定。在存在病态数据偏多的研究中有较大的实用价值。



## 1.7 逻辑回归(二分类)

> sigmoid函数：将输入转化成[0,1]之间
>
> x=0,y=0.5

**线性回归的输入 ===sigmoid===》分类**

* 输入

  * $$
    z(w) = w_0+  w_1x_1 + ... + w_n+x_n + b = w^Tx
    $$

* 逻辑回归公式

  * $$
    h_\Theta(x) = g(\Theta^Tx) = \frac{1}{1+e^{-\Theta^Tx}}
    $$

    $$
    g(z) = \frac{1}{1+e^{-z}}
    $$

    $$
    注：g(z)为sigmoid函数，z = 回归的结果
    $$

* 输出：[0,1]区间的概率值，默认0.5作为阀值


### 1.7.1 损失函数，优化

> 与线性回归原理相同，但由于是分类问题，损失函数不同，只能通过梯度下降求解

* 对数似然损失函数：

  * $$
    cost(h_\Theta(x),y) = \begin{cases} -log(h_\Theta(x)), & \text {if y=1} \\ -log(1-h_\Theta(x))，& \text{if y=0} \end{cases}
    $$

* 完整的损失函数

  * $$
    cost(h_\Theta(x),y) = \sum_{i=1}^m-y_ilog(h_\Theta(x)) - (1-y_i)log(1-h_\Theta(x))
    $$

    $$
    注：cost损失的值越小，那么预测的类别准确度更高
    $$

    

* **实例**
  * 预测属于1的概率值
  * 阈值：0.5

| 数据结果 | 线性回归的预测 |
| -------- | -------------- |
| 1        | 0.6            |
| 0        | 0.1            |
| 0        | 0.51           |
| 1        | 0.7            |

* 四个损失值相加计算
  $$
  1log(0.6) + 0log(0.1)+0log(0.51)+ 1log(0.7)
  $$

  $$
  值越小，正确率越高
  $$

  

* 损失函数

  * 均方误差（不存在多个局部最低点），只有一个最小值
  * 对数似然损失（多个局部最小值，目前无解）
    * 改善
      * 1.多次随机初始化（多次比较最小结果）
      * 2.求解过程中，调整学习率

### 1.7.2 逻辑回归API

* ```markdown
  * sklearn.linear_model.LogisticRegression(
      penalty= 'l2',
      C = 1.0
  	)
  * 参数
  	* penalty：正则化
  	* C：正则化力度
  * 结果
  	* Logistic：回归分类器
  	* coef_：回归系数
  ```

### 1.7.3 总结

* ​	应用
  * 广告点击率预测
  * 是否患病
  * 金融诈骗
  * 是否为虚假账号
* 优点
  * 适合需要得到一个分类概率的场景，简单，速度快
* 缺点
  * 不好处理多分类问题

## 1.8 逻辑回归和朴素贝叶斯比较

* 先验概率P(c)
  * P(f1,f2,f3|c)P(c)
* **判别模型(无先验概率)**
  * 逻辑回归
    * 二分类问题
    * 应用场景
      * 癌症
    * 参数
      * 正则化力度 
      * 得出结果有概率解释
* **生成模型（有先验概率）**
  * 朴素贝叶斯
    * 多分类问题
      * 文本分类、
    * 参数
      * 得出结果有概率解释

```markdown
**依据：
  判别模型：有先验概率（在某个条件下的概率）
  生成模型：无先验概率
**
* 判别模型
	* k-近邻
	* 决策树
	* 随机森林
	* 神经网络
* 生成模型
	* 朴素贝叶斯
```



## 1.9 k-means

> 非监督学习：聚类

### 1.9.1 步骤

1. `随机`设置ｋ个特征空间内的点作为初始的聚类中心
2. 对于其他每个点计算到ｋ个中心的距离，`未知的点选择最近的一个聚类中心点作为标记类别`
3. 接着对着标记的聚类中心之后，`重新计算出每个聚类的中心点`（平均值）
4. 如果计算得出的新中心点和原中心点一样，那么结束，否则进行第二步过程

### 1.9.2 算法API

```markdown
* sklearn.cluster.KMeans(
		n_clusters=8,
		init='k-means++',
	)
	* k-means聚类
    	* n_clusters：开始的聚类中心数量
    	* init：初始化方法，默认为'ｋ-means++'
    	* labels_：默认标记的类型，可以和真实值比较（不是值比较）
```

### 1.9.3 性能评估指标

- 轮廓系数：

  > 轮廓系数[-1,1]

  - $$
    sc_i = \frac{b_i-a_i}{max(b_ia_i)}
    $$

    $$
    注：对于每个点i为已聚类数据中样本，\\ 【ｂ_i为ｉ到其他族群】的所有样本的距离最小值，\\【a_i为ｉ本身簇】的距离平均值\\最终计算出所有的样本点的轮廓系数平均值
    $$

- 考虑极端值

  - ｂ_i  远大于ａ_i：１　完美
  - ａ_i 远大于ｂ_i：-1      最差

* **性能评估指标api**

```
* sklearn.metrics.sihouette_score(x,labels)
	* 计算所有样本的平均轮廓系数
		* ｘ:特征值
		* labels：被聚类标记的目标值
```

### 1.9.4 总结

> 注意：聚类一般做在分类前

- 特点
  - 采用迭代式算法，直观易懂并且非常实用
- 缺点
  - 容易收敛到局部最优解（多次聚类）

